{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pse estimation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKzdavwo2ak1"
      },
      "source": [
        "import os\r\n",
        "import cv2\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bBctCP3BDv"
      },
      "source": [
        "# Specify the paths for the 2 files\r\n",
        "\r\n",
        "cwd = os.getcwd()\r\n",
        "protoFile = \"{}/pose_deploy_linevec_faster_4_stages.prototxt\".format(cwd)\r\n",
        "weightsFile = \"{}/pose_iter_440000.caffemodel\".format(cwd)\r\n",
        "\r\n",
        "# Read the network into Memory\r\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqqf0gI7uqli"
      },
      "source": [
        "#구글 드라이브 연결\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2WH6DbJ3HVb"
      },
      "source": [
        "# Read image\r\n",
        "frame = cv2.imread(\"test1.png\")\r\n",
        "print(frame.shape)\r\n",
        "# Specify the input image dimensions\r\n",
        "inWidth = 299\r\n",
        "inHeight = 299\r\n",
        "frameWidth = frame.shape[1]\r\n",
        "frameHeight = frame.shape[0]\r\n",
        "threshold = 0.1\r\n",
        "\r\n",
        "nPoints = 10\r\n",
        "# Forward training set into OpenPose model to generate output\r\n",
        "inWidth = 299\r\n",
        "inHeight = 299\r\n",
        "# Prepare the frame to be fed to the network\r\n",
        "inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=True, crop=False)\r\n",
        " \r\n",
        "# Set the prepared object as the input blob of the network\r\n",
        "net.setInput(inpBlob)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmCHgumh33lT"
      },
      "source": [
        "output = net.forward()\r\n",
        "#output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKWsF4k637OU"
      },
      "source": [
        "H = output.shape[2]\r\n",
        "W = output.shape[3]\r\n",
        "print(H,W)\r\n",
        "# Empty list to store the detected keypoints\r\n",
        "points = []\r\n",
        "nPoints = 15\r\n",
        "for i in [0,1,2,3,4,5,6,7,8]:#수정 사항\r\n",
        "    # confidence map of corresponding body's part.\r\n",
        "    probMap = output[0, i, :, :]\r\n",
        " \r\n",
        "    # Find global maxima of the probMap.\r\n",
        "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\r\n",
        "    # Scale the point to fit on the original image\r\n",
        "    x = (frameWidth * point[0]) / W#수정사항\r\n",
        "    y = (frameHeight * point[1]) / H\r\n",
        " \r\n",
        "    if (prob > threshold):\r\n",
        "        cv2.circle(frame, (int(x), int(y)), 15, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\r\n",
        "        cv2.putText(frame, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 3, lineType=cv2.LINE_AA)\r\n",
        " \r\n",
        "        # Add the point to the list if the probability is greater than the threshold\r\n",
        "        points.append((int(x), int(y)))\r\n",
        "    else :\r\n",
        "        points.append(None)\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "cv2_imshow(frame)\r\n",
        "cv2.waitKey(0)\r\n",
        "cv2.destroyAllWindows()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NESkwD67dJm"
      },
      "source": [
        "POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7]]#, [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] \r\n",
        "for pair in POSE_PAIRS:\r\n",
        "    partA = pair[0]\r\n",
        "    partB = pair[1]\r\n",
        " \r\n",
        "    if points[partA] and points[partB]:\r\n",
        "        cv2.line(frame, points[partA], points[partB], (0, 255, 0), 3)\r\n",
        "cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHb4hsr-v4NK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "cwd = os.getcwd()\r\n",
        "# Specify the paths for the 2 files\r\n",
        "protoFile = \"{}/pose_deploy_linevec_faster_4_stages.prototxt\".format(cwd)\r\n",
        "weightsFile = \"{}/pose_iter_160000.caffemodel\".format(cwd)\r\n",
        "nPoints = 15\r\n",
        "POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\r\n",
        "  \r\n",
        " # Read the network into Memory\r\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\r\n",
        "frameWidth = 640\r\n",
        "frameHeight = 480\r\n",
        "threshold = 0.1\r\n",
        "# Forward training set into OpenPose model to generate output\r\n",
        "inWidth = 299\r\n",
        "inHeight = 299\r\n",
        "m = train_images.shape[0]\r\n",
        "train_outputs = np.zeros((1,44,38,38))\r\n",
        "for i in range(m):\r\n",
        "    inpBlob = cv2.dnn.blobFromImage(train_images[i], 1.0/255,    (inWidth, inHeight),(0, 0, 0), swapRB=True, crop=False)\r\n",
        "    net.setInput(inpBlob)\r\n",
        "    output = net.forward()\r\n",
        "    train_outputs = np.vstack((train_outputs,output))\r\n",
        "   \r\n",
        "   \r\n",
        "outputs = np.delete(train_outputs,(0),axis=0)\r\n",
        "H = train_outputs.shape[2]\r\n",
        "W = train_outputs.shape[3]\r\n",
        "print(train_outputs.shape)\r\n",
        "# Generate keypoints for training set\r\n",
        "m = 973\r\n",
        "H = 38\r\n",
        "W = 38\r\n",
        "train_points = np.zeros((m,15,2))\r\n",
        "for sample in range(m):\r\n",
        "   for i in range(nPoints):\r\n",
        "       # confidence map of corresponding body's part.\r\n",
        "       probMap = train_outputs[sample, i, :, :]\r\n",
        "# Find global maxima of the probMap.\r\n",
        "       minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\r\n",
        "# Scale the point to fit on the original image\r\n",
        "       x = (frameWidth * point[0]) / W\r\n",
        "       y = (frameHeight * point[1]) / H\r\n",
        "       if prob > threshold :\r\n",
        "           train_points[sample,i,0] = int(x)\r\n",
        "           train_points[sample,i,1] = int(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Iyn3R8v8fx"
      },
      "source": [
        "# Processed images with sticks on original image\r\n",
        "train_processed = np.copy(train_images).astype(np.uint8)\r\n",
        "for sample in range(m):\r\n",
        "   for point in range(nPoints):\r\n",
        "     if train_points[sample,point,0] !=0 and train_points[sample,point,1] !=0 :\r\n",
        "       cv2.circle(train_processed[sample], (int(train_points[sample,point,0]), int(train_points[sample,point,1])), 10, (255,255,0), thickness=-1, lineType=cv2.FILLED)\r\n",
        "     \r\n",
        "     # draw lines \r\n",
        "     for pair in POSE_PAIRS:\r\n",
        "       partA = pair[0]\r\n",
        "       partB = pair[1]\r\n",
        "if train_points[sample,partA,0] != 0 and train_points[sample,partA,1] != 0 and train_points[sample,partB,0] != 0 and train_points[sample,partB,1] != 0:\r\n",
        "          cv2.line(train_processed[sample], (int(train_points[sample,partA,0]),int(train_points[sample,partA,1]))\r\n",
        "                   , (int(train_points[sample,partB,0]),int(train_points[sample,partB,1])), (255,255,0), 3)\r\n",
        "# Processed images with sticks on a black background\r\n",
        "train_processed_grey = np.zeros((m,train_images.shape[1],train_images.shape[2],1)).astype(np.uint8)\r\n",
        "for sample in range(m):\r\n",
        "   for point in range(nPoints):\r\n",
        "     if train_points[sample,point,0] !=0 and train_points[sample,point,1] !=0 :\r\n",
        "       cv2.circle(train_processed_grey[sample], (int(train_points[sample,point,0]), int(train_points[sample,point,1])), 10, (1), thickness=50, lineType=cv2.FILLED)\r\n",
        "     \r\n",
        "     # draw lines \r\n",
        "     for pair in POSE_PAIRS:\r\n",
        "       partA = pair[0]\r\n",
        "       partB = pair[1]\r\n",
        "if train_points[sample,partA,0] != 0 and train_points[sample,partA,1] != 0 and train_points[sample,partB,0] != 0 and train_points[sample,partB,1] != 0:\r\n",
        "          cv2.line(train_processed_grey[sample], (int(train_points[sample,partA,0]),int(train_points[sample,partA,1]))\r\n",
        "                   , (int(train_points[sample,partB,0]),int(train_points[sample,partB,1])), (1), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCSoA1Nv_oZ"
      },
      "source": [
        "!pip install -Uqq fastbook\r\n",
        "import fastai\r\n",
        "from fastai.metrics import error_ratefrom torchvision.models import *\r\n",
        "import pretrainedmodels\r\n",
        "from fastai.callbacks.tracker import SaveModelCallback\r\n",
        "from fastai.vision import *\r\n",
        "from fastai.vision.models import *\r\n",
        "from fastai.vision.learner import model_meta\r\n",
        "bs = 8\r\n",
        "# Importing of dataset\r\n",
        "data = ImageDataBunch.from_folder(base_dir,train='train',valid='val', ds_tfms = get_transforms(), size =299, bs=bs).normalize(imagenet_stats)\r\n",
        "#ResNet-50\r\n",
        "models.resnet50\r\n",
        "#ResNeXt-101\r\n",
        "def resnext101_64x4d(pretrained=True):\r\n",
        "    pretrained = 'imagenet' if pretrained else None\r\n",
        "    model = pretrainedmodels.resnext101_64x4d(pretrained=pretrained)\r\n",
        "    all_layers = list(model.children())\r\n",
        "    return nn.Sequential(*all_layers[0], *all_layers[1:])\r\n",
        "#PASNet-5\r\n",
        "def identity(x): return x\r\n",
        "def pnasnet5large(pretrained=True):    \r\n",
        "    pretrained = 'imagenet' if pretrained else None\r\n",
        "    model = pretrainedmodels.pnasnet5large(pretrained=pretrained, num_classes=1000) \r\n",
        "    model.logits = identity\r\n",
        "    return nn.Sequential(model)\r\n",
        "# Training of model\r\n",
        "learn = cnn_learner(data, model, metrics = accuracy)\r\n",
        "learn.fit(20,callbacks=[SaveModelCallback(learn,monitor='accuracy',every=\"improvement\",name =\"top_acc\")])\r\n",
        "learn.lr_find()\r\n",
        "learn.recorder.plot()\r\n",
        "learn.unfreeze()\r\n",
        "learn.fit_one_cycle(5,callbacks=[SaveModelCallback(learn,monitor='accuracy',every=\"improvement\",name=\"top_acc_1\")])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}